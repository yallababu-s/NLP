{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cf9b38-f737-4f99-86d6-677a510f06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import emoji\n",
    "import datetime\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7180a9ea-5f8a-4efd-ac2e-4de3e124e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\YAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\YAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dcfec6-1116-4192-967f-3316f9f441c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time. time()\n",
    "plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':150})\n",
    "#file=pd.read_csv(\"rrr.txt\", sep = \"\\t\",encoding='utf8')\n",
    "#df = pd.read_csv('rrr.txt', delimiter= '\\s', index_col=False)\n",
    "#filename='rr.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb885e2-fc42-4a61-8a12-a0557a3f1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhatsappFilrt:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def PreProcessing(self,filename):\n",
    "        df = pd.read_csv(filename,header=None,error_bad_lines=False,encoding='utf8')\n",
    "        if(len(df.columns)==2):\n",
    "            df=df.drop(0)\n",
    "            df=df.dropna(axis=0)\n",
    "            df.columns=['Date','Text']\n",
    "            Message=df[\"Text\"].str.split(\"-\",n=1,expand=True)\n",
    "            df[\"Time\"]=Message[0]\n",
    "            Message1=Message[1].str.split(\":\",n=1,expand=True)\n",
    "            df[\"Name\"]=Message1[0]\n",
    "            df[\"Text\"]=Message1[1]\n",
    "            df=df[[\"Date\",\"Time\",\"Name\",\"Text\"]]\n",
    "            df=df.dropna(axis=0)\n",
    "        elif(len(df.columns)==4):\n",
    "            df=df.drop([2,3],axis=1)\n",
    "            df=df.dropna(axis=0)\n",
    "            df=df.drop(0)\n",
    "            df.columns=['Date','Text']\n",
    "            Message=df[\"Text\"].str.split(\"-\",n=1,expand=True)\n",
    "            df[\"Time\"]=Message[0]\n",
    "            Message1=Message[1].str.split(\":\",n=1,expand=True)\n",
    "            df[\"Name\"]=Message1[0]\n",
    "            df[\"Text\"]=Message1[1]\n",
    "            df=df[[\"Date\",\"Time\",\"Name\",\"Text\"]]\n",
    "            df=df.dropna(axis=0)\n",
    "        else:\n",
    "            print(len(df.columns))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8384b63e-cb9e-4abf-8df8-8e5d37b90a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(self,text):\n",
    "        allchars = [str for str in text.decode('utf-8')]\n",
    "        emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "        clean_text = ' '.join([str for str in text.decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "        return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c222444-77ad-44e3-a114-4cb26bd60438",
   "metadata": {},
   "outputs": [],
   "source": [
    " def deEmojify(self,inputString):\n",
    "        return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4743e9ec-1fb1-466f-af09-ad56f44e87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_Emoji(self,text,dataset):\n",
    "        for i, x in enumerate(text):\n",
    "            if(x==0):\n",
    "                dataset['Text'][i]='Emoji'\n",
    "        return dataset\n",
    "def TalkerChecking(self,unique_Frequency_Talker,flirt_words,Talker_Filter_list):\n",
    "        for i in unique_Frequency_Talker.index:\n",
    "            if i in flirt_words:\n",
    "                Talker_Filter_list.append(unique_Frequency_Talker[unique_Frequency_Talker.index==i])\n",
    "        return Talker_Filter_list\n",
    "def LesserChecking(self,unique_Frequency_Lesser,flirt_words,Less_Filter_list):\n",
    "        for i in unique_Frequency_Lesser.index:\n",
    "            if i in flirt_words:\n",
    "                Less_Filter_list.append(unique_Frequency_Lesser[unique_Frequency_Lesser.index==i])\n",
    "        return  Less_Filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ef0317-8748-4f5a-9de8-7722380b032a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def Wholeprocess(self,dataset):\n",
    "        result=[]\n",
    "        result1=[]\n",
    "        dataset=dataset.dropna(subset=['Date','Time','Name','Text'])\n",
    "        dataset[\"Text\"]=dataset['Text'].apply(lambda x : obj.give_emoji_free_text(str(x).encode('utf8')))\n",
    "        dataset['Name']=dataset['Name'].apply(lambda x : obj.deEmojify(str(x).lower()))\n",
    "        dataset['TW']=dataset['Text'].str.split().str.len()\n",
    "\n",
    "        dataset.index=range(dataset.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "        dataset=obj.assign_Emoji(dataset.TW,dataset)\n",
    "\n",
    "\n",
    "        dataset.index=range(dataset.shape[0])\n",
    "        total_word_file=dataset['TW'].sum()\n",
    "\n",
    "        \"Finding top 3 chats with  words count \"\n",
    "        chater=dataset['Name'].value_counts().head(7).to_dict()\n",
    "\n",
    "\n",
    "        \"Finding More\"\n",
    "        Talker=dataset['Name'].value_counts().idxmax()\n",
    "        Talker1=Talker.upper()\n",
    "        print(\"More Talktative:\", Talker.upper())\n",
    "        result.append(Talker1)\n",
    "        Less_Talker=dataset['Name'].value_counts().idxmin()\n",
    "        Less_Talker1=Less_Talker.upper()\n",
    "        print(\"Less Talktative:\", Less_Talker.upper())\n",
    "        result.append(Less_Talker1)\n",
    "\n",
    "\n",
    "        Talker_chat= pd.DataFrame(dataset[dataset.Name==Talker])\n",
    "        Less_chat= pd.DataFrame(dataset[dataset.Name==Less_Talker])\n",
    "\n",
    "        unique_Frequency_Talker= pd.DataFrame(Talker_chat['Text'].str.split(' ', expand =True).stack().value_counts())\n",
    "        unique_Frequency_Lesser= pd.DataFrame(Less_chat['Text'].str.split(' ', expand =True).stack().value_counts())\n",
    "\n",
    "        unique_Frequency_Talker['Usage of word']=(unique_Frequency_Talker[0]/Talker_chat['TW'].sum())*100\n",
    "        unique_Frequency_Lesser['Usage of word']=(unique_Frequency_Lesser[0]/Less_chat['TW'].sum())*100\n",
    "\n",
    "\n",
    "        flirt_words=['kiss','hug','date', 'cute',\n",
    "                       'beautiful', 'sexy', 'hot','adorable','uma', 'darling',\n",
    "                       'fuck','porn', 'x', 'sex', 'matter', 'nipple', 'virgin', 'sperm',\n",
    "                       'seduce', 'condom','kk']\n",
    "\n",
    "            #Extracting flirt word\n",
    "        Talker_Filter_list=[]\n",
    "\n",
    "        Less_Filter_list=[]\n",
    "\n",
    "\n",
    "            #print(Talker,Talker_Filter_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #obj.TalkerChecking(unique_Frequency_Talker,flirt_words,Talker_Filter_list)\n",
    "        try:\n",
    "\n",
    "            Talker_Filter_list=pd.concat(obj.TalkerChecking(unique_Frequency_Talker,flirt_words,Talker_Filter_list))\n",
    "            result1.append(Talker_Filter_list)\n",
    "        except ValueError:\n",
    "            print(\"Wonderfull no flirting by {}\".format(Talker.upper()))\n",
    "\n",
    "        try:\n",
    "            Talker_Filter_list.columns=['Repeated_count','Frequency_Value']\n",
    "            Talker_Filter_list['Flirt_Frequency']=(Talker_Filter_list['Repeated_count']/len(flirt_words))*100\n",
    "            Talker_out=round(Talker_Filter_list['Flirt_Frequency'].sum()/Talker_Filter_list.shape[0],2)\n",
    "            print(\"Flirting Percentage of{}:\".format(Talker.upper()), Talker_out,\"%\")\n",
    "            result.append(Talker_out)\n",
    "\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "\n",
    "        #obj.LesserChecking(unique_Frequency_Lesser,flirt_words,Less_Filter_list)\n",
    "            #print(Less_Talker,Less_Filter_list)\n",
    "\n",
    "        try:\n",
    "             Less_Filter_list= pd.concat(obj.LesserChecking(unique_Frequency_Lesser,flirt_words,Less_Filter_list))\n",
    "             result1.append(Less_Filter_list)\n",
    "        except ValueError:\n",
    "            print(\"Wonderfull no flirting by {}\".format(Less_Talker.upper()))\n",
    "\n",
    "        try:\n",
    "            Less_Filter_list.columns=['Repeated_count','Frequency_Value']\n",
    "            Less_Filter_list['Flirt_Frequency']=(Less_Filter_list['Repeated_count']/len(flirt_words))*100\n",
    "            Less_out=round(Less_Filter_list['Flirt_Frequency'].sum()/Less_Filter_list.shape[0],2)\n",
    "            print(\"Flirting Percentage of{}:\".format(Less_Talker.upper()), Less_out,\"%\")\n",
    "            result.append(Less_out)\n",
    "\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return result, result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f576bb7-40d7-45eb-adac-34b65937ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statsWhatsApp(self,dataset):\n",
    "        stats=[]\n",
    "        stats1=[]\n",
    "        date_chart = pd.DataFrame(dataset[\"Date\"].value_counts())\n",
    "        plt.clf()\n",
    "        #plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':150})\n",
    "        ax1=sns.countplot(x='Date',hue='Name', data=dataset)\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "        ax1.figure.savefig(\"static/output_image/date_chart_name.png\")\n",
    "        #stats.append(date_chart)\n",
    "        most_active_date= dataset[\"Date\"].value_counts().idxmax()\n",
    "        stats.append(most_active_date)\n",
    "        week_days= ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday']\n",
    "        l=list(map(int, most_active_date.split('/')))\n",
    "        day=datetime.date(l[2],l[0],l[1]).weekday()\n",
    "        active_week_day=week_days[day]\n",
    "        stats.append(active_week_day)\n",
    "        active_hour_of_day= dataset[\"Time\"].value_counts().idxmax()\n",
    "        stats.append(active_hour_of_day)\n",
    "        avg_no_of_msgs_per_day = round(dataset[\"Date\"].count() / dataset[\"Date\"].nunique())\n",
    "        stats.append(avg_no_of_msgs_per_day )\n",
    "        ##########################\n",
    "        media =dataset[dataset[\"Text\"] == \" <media omitted>\"]\n",
    "        if len(media):\n",
    "            media_count = pd.DataFrame(media[\"Name\"].value_counts())\n",
    "            #media_count.columns=['Count']\n",
    "            media_share_freak = media[\"Name\"].value_counts().idxmax()\n",
    "            #stats1.append(media_count)\n",
    "            stats1.append(media_share_freak)\n",
    "        else:\n",
    "            media_count = \"\"\n",
    "            media_share_freak = \"No Media were shared\"\n",
    "            #stats1.append(media_count)\n",
    "            stats1.append(media_share_freak)\n",
    "\n",
    "        ####################\n",
    "        message_deleted =dataset[dataset[\"Text\"] == \" this message was deleted\"]\n",
    "        if len(message_deleted):\n",
    "            msg_deleted_count = pd.DataFrame(message_deleted[\"Name\"].value_counts())\n",
    "            #msg_deleted_count.columns=['Count']\n",
    "            msg_deleting_freak = message_deleted[\"Name\"].value_counts().idxmax()\n",
    "            #stats1.append(msg_deleted_count)\n",
    "            stats1.append(msg_deleting_freak)\n",
    "        else:\n",
    "            msg_deleted_count = \"\"\n",
    "            msg_deleting_freak = \"No message was deleted\"\n",
    "            #stats1.append(msg_deleted_count)\n",
    "            stats1.append(msg_deleting_freak)\n",
    "        #################################\n",
    "        voice_call=dataset[dataset[\"Text\"]==\" missed voice call\"]\n",
    "        if len(voice_call):\n",
    "            voice_call_count = pd.DataFrame(voice_call[\"Name\"].value_counts())\n",
    "            #voice_call_count.columns=['Count']\n",
    "            voice_calling_freak = voice_call[\"Name\"].value_counts().idxmax()\n",
    "            #stats1.append(voice_call_count)\n",
    "            stats1.append(voice_calling_freak)\n",
    "        else:\n",
    "            voice_call_count = \"\"\n",
    "            voice_calling_freak = \"No Missed Voice Call\"\n",
    "            #stats1.append(voice_call_count)\n",
    "            stats1.append(voice_calling_freak)\n",
    "        ######################################\n",
    "        video_call=dataset[dataset[\"Text\"]==\" missed video call\"]\n",
    "        if len(video_call):\n",
    "            video_call_count = pd.DataFrame(video_call[\"Name\"].value_counts())\n",
    "            #video_call_count.columns=['Count']\n",
    "            video_calling_freak = video_call[\"Name\"].value_counts().idxmax()\n",
    "            #stats1.append(video_call_count)\n",
    "            stats1.append(video_calling_freak)\n",
    "        else:\n",
    "            video_call_count = \"\"\n",
    "            video_calling_freak = \"No Missed Video Call\"\n",
    "            #stats1.append(video_call_count)\n",
    "            stats1.append(video_calling_freak)\n",
    "        return date_chart,media_count,msg_deleted_count,voice_call_count,video_call_count,stats, stats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a3a92b-f7b1-484a-aaef-8ef0a1a20d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def sentimentalAnalysis(self,data):\n",
    "\n",
    "        #downloading vader_lexicon for the process\n",
    "\n",
    "        \"Importing Necessary Packeage\"\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "        \"Deleting null pr empty value\"\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        \"Checking for a comment\"\n",
    "        #sid.polarity_scores(data['Text'][93])\n",
    "\n",
    "        \"Creating respective columns\"\n",
    "\n",
    "        data['scores'] = data['Text'].apply(lambda commentText: sid.polarity_scores(commentText))\n",
    "        data['compound']  = data['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "        data['Negtive']  = data['scores'].apply(lambda score_dict: score_dict['neg'])\n",
    "        data['Postive']  = data['scores'].apply(lambda score_dict: score_dict['pos'])\n",
    "        data['Neutral']  = data['scores'].apply(lambda score_dict: score_dict['neu'])\n",
    "\n",
    "        \"Creating final pos or neg using compound score\"\n",
    "        data['comp_score'] = data['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "        plt.clf()\n",
    "        comp=sns.countplot(x = 'comp_score', hue = 'Name', data = data, palette = 'magma')\n",
    "        comp.figure.savefig(\"static/output_image/posneg.png\")\n",
    "        \"Checking how many pos and neg\"\n",
    "        posneg=pd.DataFrame(data['comp_score'].value_counts())\n",
    "        return posneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f449f304-7fda-49e7-9b92-a95c0cc5ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicModelling(self,dataset):\n",
    "        results=[]\n",
    "        tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')# Assign the value for max and min allowance stopwords\n",
    "        dtm = tfidf.fit_transform(dataset['Text'])# Transforming commentText to tfdif formula\n",
    "        nmf_model = NMF(n_components=5,random_state=42)#Assigning number of topics\n",
    "        nmf_model.fit(dtm)#Tranforming Transformed commentText formula to NMF .This stores the output of 5 topic modelling topics\n",
    "        #cc=nmf_model.components_\n",
    "        for index,topic in enumerate(nmf_model.components_):# Now we are printing Top 10 words under each Topic\n",
    "            #print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "            result=([tfidf.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "            #print(result)\n",
    "            results.append([index,result])\n",
    "        #print(results)\n",
    "        topic_results = nmf_model.transform(dtm)\n",
    "        dataset['Topic'] = topic_results.argmax(axis=1)#Assign respective topics to each commentTent\n",
    "\n",
    "\n",
    "        topicmodelling=pd.DataFrame(results)\n",
    "        return topicmodelling,dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4eb5317-cf02-44c1-8d4e-c9047b560ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0.09181737899780273\n"
     ]
    }
   ],
   "source": [
    "   def wordcloud(self,dataset):\n",
    "        comment_words = []\n",
    "        stopword = stopwords.words('english')\n",
    "        stopword.extend(['omitted', 'voice','missed','call','video','deleted','media','message'])\n",
    "        wordcloudss=\"This function saves image\"\n",
    "        dataset.index=range(dataset.shape[0])\n",
    "        for i in range(1,len(dataset)):\n",
    "            comment_words.append(dataset['Text'][i])\n",
    "        vv=\" \".join(comment_words)\n",
    "        #plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':150})\n",
    "        plt1.clf()\n",
    "        wordcloud = WordCloud(width = 800, height = 800,\n",
    "                                  background_color ='white',\n",
    "                                  stopwords = stopword,\n",
    "                                  min_font_size = 10).generate(vv)\n",
    "        #plt.figure(figsize = (9, 7), facecolor = None)\n",
    "        plt1.imshow(wordcloud)\n",
    "        plt1.axis(\"off\")\n",
    "        #plt.tight_layout(pad = 0)\n",
    "        plt1.savefig('static/output_image/wordcloud.png')\n",
    "        #plt.show()\n",
    "        #print(\"Successfully created\")\n",
    "        return wordcloudss\n",
    "\n",
    "obj=WhatsappFilrt()\n",
    "\n",
    "end = time. time()\n",
    "print('Execution Time:',end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eedbce-12b5-4858-ac43-c080e925ddfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
