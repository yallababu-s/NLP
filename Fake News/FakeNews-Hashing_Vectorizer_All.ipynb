{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc40218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed202ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 26381)\t-0.1336306209562122\n",
      "  (1, 46353)\t0.1336306209562122\n",
      "  (1, 76282)\t-0.1336306209562122\n",
      "  (1, 124604)\t-0.1336306209562122\n",
      "  (1, 271872)\t0.2672612419124244\n",
      "  (1, 354766)\t0.1336306209562122\n",
      "  (1, 355578)\t-0.1336306209562122\n",
      "  (1, 380136)\t-0.2672612419124244\n",
      "  (1, 399927)\t-0.1336306209562122\n",
      "  (1, 413315)\t-0.2672612419124244\n",
      "  (1, 421751)\t-0.2672612419124244\n",
      "  (1, 452780)\t-0.1336306209562122\n",
      "  (1, 506429)\t-0.5345224838248488\n",
      "  (1, 612563)\t-0.1336306209562122\n",
      "  (1, 615897)\t0.1336306209562122\n",
      "  (1, 626851)\t0.1336306209562122\n",
      "  (1, 639862)\t0.1336306209562122\n",
      "  (1, 691517)\t-0.1336306209562122\n",
      "  (1, 740856)\t-0.1336306209562122\n",
      "  (1, 777362)\t-0.2672612419124244\n",
      "  (1, 798576)\t-0.1336306209562122\n",
      "  (1, 907820)\t0.2672612419124244\n",
      "  (1, 1039472)\t-0.1336306209562122\n",
      "  (2, 14361)\t-0.3333333333333333\n",
      "  (2, 81229)\t0.3333333333333333\n",
      "  :\t:\n",
      "  (4243, 924171)\t0.08478501284163323\n",
      "  (4243, 934801)\t0.028261670947211076\n",
      "  (4243, 935153)\t0.028261670947211076\n",
      "  (4243, 935275)\t0.028261670947211076\n",
      "  (4243, 939307)\t-0.1130466837888443\n",
      "  (4243, 946139)\t-0.028261670947211076\n",
      "  (4243, 949760)\t0.028261670947211076\n",
      "  (4243, 953107)\t0.028261670947211076\n",
      "  (4243, 958756)\t-0.028261670947211076\n",
      "  (4243, 958992)\t-0.028261670947211076\n",
      "  (4243, 963256)\t-0.028261670947211076\n",
      "  (4243, 963865)\t-0.028261670947211076\n",
      "  (4243, 975215)\t0.028261670947211076\n",
      "  (4243, 982957)\t0.14130835473605538\n",
      "  (4243, 993015)\t0.028261670947211076\n",
      "  (4243, 1004174)\t0.028261670947211076\n",
      "  (4243, 1004879)\t-0.028261670947211076\n",
      "  (4243, 1009763)\t0.05652334189442215\n",
      "  (4243, 1010115)\t-0.028261670947211076\n",
      "  (4243, 1017691)\t-0.028261670947211076\n",
      "  (4243, 1026684)\t-0.028261670947211076\n",
      "  (4243, 1027460)\t-0.028261670947211076\n",
      "  (4243, 1038430)\t0.028261670947211076\n",
      "  (4243, 1039483)\t-0.028261670947211076\n",
      "  (4243, 1044795)\t0.05652334189442215\n",
      "Number of feature names: 1048576\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('news.csv', index_col=None)\n",
    "dataset=df.drop(\"Unnamed: 0\",axis=1)\n",
    "y=dataset[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "Hashing_Vectorizer =HashingVectorizer(stop_words='english')\n",
    "Hashing_train =Hashing_Vectorizer.fit_transform(X_train)\n",
    "print(Hashing_train)\n",
    "Hashing_test =Hashing_Vectorizer.transform(X_test)\n",
    "feature_names = [f'feature_{i}' for i in range(Hashing_train.shape[1])]\n",
    "\n",
    "# Print number of feature names\n",
    "print(\"Number of feature names:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f637eec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 88\u001b[0m\n\u001b[0;32m     81\u001b[0m hash_test \u001b[38;5;241m=\u001b[39m hash_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Call each algorithm function with the training and test data\u001b[39;00m\n\u001b[0;32m     84\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: logistic_regression(hash_train, y_train, hash_test),\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear SVM\u001b[39m\u001b[38;5;124m'\u001b[39m: linear_svm(hash_train, y_train, hash_test),\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKernel SVM\u001b[39m\u001b[38;5;124m'\u001b[39m: kernel_svm(hash_train, y_train, hash_test),\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaive Bayes\u001b[39m\u001b[38;5;124m'\u001b[39m: naive_bayes(hash_train, y_train, hash_test), \n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK-Nearest Neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: k_nearest_neighbors(hash_train, y_train, hash_test),\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m'\u001b[39m: decision_tree(hash_train, y_train, hash_test),\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: random_forest(hash_train, y_train, hash_test)\n\u001b[0;32m     92\u001b[0m }\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, (classifier, accuracy, report, cm) \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mnaive_bayes\u001b[1;34m(X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnaive_bayes\u001b[39m(X_train, y_train, X_test):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Fitting Naive Bayes to the Training set\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[1;32m---> 50\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     51\u001b[0m     accuracy, report, cm \u001b[38;5;241m=\u001b[39m evaluate_classifier(classifier, X_test, y_test)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier, accuracy, report, cm\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:263\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_fit(\n\u001b[0;32m    264\u001b[0m     X, y, np\u001b[38;5;241m.\u001b[39munique(y), _refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m    265\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:423\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    422\u001b[0m first_call \u001b[38;5;241m=\u001b[39m _check_partial_fit_first_call(\u001b[38;5;28mself\u001b[39m, classes)\n\u001b[1;32m--> 423\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39mfirst_call)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1304\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1305\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1306\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1307\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1308\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1309\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1310\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1311\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1312\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1313\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1314\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:971\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[0;32m    970\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 971\u001b[0m     array \u001b[38;5;241m=\u001b[39m _ensure_sparse_format(\n\u001b[0;32m    972\u001b[0m         array,\n\u001b[0;32m    973\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m    974\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    975\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    976\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m    977\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m    978\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    979\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    982\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    983\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D input, got input with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:595\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(sparse_container, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m     padded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m input_name \u001b[38;5;28;01mif\u001b[39;00m input_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse data was passed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadded_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but dense data is required. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.toarray()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to convert to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    598\u001b[0m     )\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accept_sparse, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accept_sparse) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, cm\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test):\n",
    "    # Fitting Logistic Regression to the Training set\n",
    "    classifier = LogisticRegression(random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def linear_svm(X_train, y_train, X_test):\n",
    "    # Fitting Linear SVM to the Training set\n",
    "    classifier = SVC(kernel='linear', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def kernel_svm(X_train, y_train, X_test):\n",
    "    # Fitting Kernel SVM to the Training set\n",
    "    classifier = SVC(kernel='rbf', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def naive_bayes(X_train, y_train, X_test):\n",
    "    # Fitting Naive Bayes to the Training set\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def k_nearest_neighbors(X_train, y_train, X_test):\n",
    "    # Fitting K-Nearest Neighbors to the Training set\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test):\n",
    "    # Fitting Decision Tree to the Training set\n",
    "    classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def random_forest(X_train, y_train, X_test):\n",
    "    # Fitting Random Forest to the Training set\n",
    "    classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "# Sample usage\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Apply HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_train = hash_vectorizer.fit_transform(X_train)\n",
    "hash_test = hash_vectorizer.transform(X_test)\n",
    "\n",
    "# Call each algorithm function with the training and test data\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression(hash_train, y_train, hash_test),\n",
    "    'Linear SVM': linear_svm(hash_train, y_train, hash_test),\n",
    "    'Kernel SVM': kernel_svm(hash_train, y_train, hash_test),\n",
    "    'Naive Bayes': naive_bayes(hash_train, y_train, hash_test), \n",
    "    'K-Nearest Neighbors': k_nearest_neighbors(hash_train, y_train, hash_test),\n",
    "    'Decision Tree': decision_tree(hash_train, y_train, hash_test),\n",
    "    'Random Forest': random_forest(hash_train, y_train, hash_test)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for clf_name, (classifier, accuracy, report, cm) in models.items():\n",
    "    print(clf_name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eda46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize empty lists to store classifier names and accuracies\n",
    "classifier_names = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each classifier and store the accuracy\n",
    "for clf_name, (classifier, accuracy, _, _) in models.items():\n",
    "    classifier_names.append(clf_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a DataFrame to store the accuracy scores\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Classifier': classifier_names,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e98f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store classifier names and accuracy scores\n",
    "classifiers = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each classifier and store the accuracy scores\n",
    "for clf_name, (_, accuracy, _, _) in models.items():\n",
    "    classifiers.append(clf_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(classifiers, accuracies, color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Accuracy of Classifiers using TfidfVectorizerVectorizer')\n",
    "\n",
    "# Add accuracy scores to the end of each bar\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{acc:.2f}', \n",
    "             va='center', ha='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd88c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5b010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6b41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
